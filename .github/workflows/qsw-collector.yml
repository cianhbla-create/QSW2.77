name: QSW Collector
on:
  workflow_dispatch: {}

jobs:
  collect:
    runs-on: ubuntu-latest
    timeout-minutes: 720

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install boto3 requests numpy pandas reportlab

      - name: Show repo tree (debug)
        run: |
          echo "::group::Repo tree"
          ls -la
          find . -maxdepth 3 -type f | sed 's|^\./||'
          echo "::endgroup::"

      # ---- Fail fast if collector missing (expects repo root) ----
      - name: Verify collector path
        run: |
          test -f "actions_runner.py" || { echo "ERROR: actions_runner.py not found at repo root"; exit 1; }

      # ---- R2 preflight using your existing endpoint secret ----
      - name: R2 preflight (write a tiny status object)
        env:
          S3_ENDPOINT_URL:        ${{ secrets.R2_ENDPOINT_URL }}   # e.g. https://<acctid>.r2.cloudflarestorage.com
          S3_BUCKET:              ${{ secrets.R2_BUCKET }}
          AWS_ACCESS_KEY_ID:      ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY:  ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          python - <<'PY'
          import os, time, boto3
          from botocore.client import Config
          ep   = os.environ.get("S3_ENDPOINT_URL","").strip()
          buck = os.environ.get("S3_BUCKET","").strip()
          if not ep or not buck:
              raise SystemExit("R2 preflight: missing S3_ENDPOINT_URL or S3_BUCKET")
          s3 = boto3.client(
              "s3",
              endpoint_url=ep,
              aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
              aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
              region_name="auto",
              config=Config(signature_version="s3v4", s3={"addressing_style":"virtual"}),
          )
          key = f"_status/preflight_{int(time.time())}.txt"
          s3.put_object(Bucket=buck, Key=key, Body=b"ok")
          print("R2 preflight wrote:", f"s3://{buck}/{key}")
          PY

      # ---- RUN COLLECTOR (repo-root actions_runner.py) ----
      - name: Run collector
        env:
          # R2 creds (if your runner does internal uploads)
          S3_ENDPOINT_URL:        ${{ secrets.R2_ENDPOINT_URL }}
          S3_REGION:              "auto"
          S3_BUCKET:              ${{ secrets.R2_BUCKET }}
          S3_PREFIX:              ${{ secrets.R2_PREFIX }}
          AWS_ACCESS_KEY_ID:      ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY:  ${{ secrets.R2_SECRET_ACCESS_KEY }}

          # Experiment knobs
          QSW_TRIALS: "3000"
          QSW_TRIALS_MODE: "per_condition"
          QSW_PHASE_LEN: "8"
          QSW_MAX_STEPS: "30"
          QSW_CONDITIONS: "quantum,pseudo,deterministic"

          # QRNG tuning
          QSW_QRNG_PROVIDERS: "anu"   # or "anu,json"
          QSW_PURE: "1"
          QSW_BATCH: "4096"
          QSW_LOW_WATERMARK: "2048"
          QSW_MAX_RETRIES: "12"
          QSW_BACKOFF: "2.0"
          QSW_PURE_MAX_WAIT: "1800"

          # Tag
          QSW_RUN_NAME: "all"
        run: |
          echo "[SANITY] Launching actions_runner.py…"
          python actions_runner.py

      # ---- GitHub artifact (always upload) ----
      - name: Upload artifacts to GitHub (always)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            QSW_runs/**/_status/**
            QSW_runs/**/results_summary.csv
            QSW_runs/**/metadata.json
            QSW_runs/**/manifest.json
            QSW_runs/**/raw/**

      # ---- Cloudflare R2 upload (timestamp + run id) ----
      - name: Upload to Cloudflare R2 (timestamp + run id)
        if: ${{ always() }}
        env:
          AWS_ACCESS_KEY_ID:      ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY:  ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET:              ${{ secrets.R2_BUCKET }}
          R2_PREFIX:              ${{ secrets.R2_PREFIX }}
          R2_ENDPOINT_URL:        ${{ secrets.R2_ENDPOINT_URL }}
        run: |
          echo "[INFO] Uploading QSW_runs/* to R2…"
          TS=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          DEST="${R2_PREFIX:+$R2_PREFIX/}collector-runs/${TS}-run${{ github.run_id }}/"
          echo "[INFO] Destination: s3://$R2_BUCKET/$DEST"
          if [ -d "QSW_runs" ]; then
            aws s3 cp QSW_runs "s3://$R2_BUCKET/$DEST" --recursive \
              --endpoint-url "$R2_ENDPOINT_URL"
          else
            echo "[WARN] QSW_runs directory not found; nothing to upload."
          fi
