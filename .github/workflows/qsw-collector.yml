name: QSW Collector
on: { workflow_dispatch: {} }

jobs:
  collect:
    runs-on: ubuntu-latest
    timeout-minutes: 720
    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with: { python-version: "3.11" }

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install boto3 requests numpy pandas reportlab

      - name: R2 preflight (write a tiny status object)
        env:
          S3_ENDPOINT_URL: ${{ secrets.R2_ENDPOINT_URL }}   # https://<acctid>.r2.cloudflarestorage.com
          S3_BUCKET:       ${{ secrets.R2_BUCKET }}
          AWS_ACCESS_KEY_ID:     ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          python - <<'PY'
          import os, time, boto3
          from botocore.client import Config
          ep   = os.environ["S3_ENDPOINT_URL"].strip().rstrip("/")
          buck = os.environ["S3_BUCKET"].strip()
          key  = f"_status/preflight_{int(time.time())}.txt"
          s3 = boto3.client(
              "s3",
              endpoint_url=ep if ep.startswith("http") else "https://" + ep,
              aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
              aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
              region_name="auto",
              config=Config(signature_version="s3v4", s3={"addressing_style":"virtual"}),
          )
          s3.put_object(Bucket=buck, Key=key, Body=b"ok")
          print("R2 preflight wrote:", f"s3://{buck}/{key}")
          PY

      - name: Run collector
        env:
          S3_ENDPOINT_URL:        ${{ secrets.R2_ENDPOINT_URL }}
          S3_REGION:              "auto"
          S3_BUCKET:              ${{ secrets.R2_BUCKET }}
          S3_PREFIX:              ${{ secrets.R2_PREFIX }}       # optional base path
          AWS_ACCESS_KEY_ID:      ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY:  ${{ secrets.R2_SECRET_ACCESS_KEY }}
          QSW_TRIALS: "3000"
          QSW_TRIALS_MODE: "per_condition"
          QSW_PHASE_LEN: "8"
          QSW_MAX_STEPS: "30"
          QSW_CONDITIONS: "quantum,pseudo,deterministic"
          QSW_QRNG_PROVIDERS: "anu"
          QSW_PURE: "1"
          QSW_BATCH: "4096"
          QSW_LOW_WATERMARK: "2048"
          QSW_MAX_RETRIES: "12"
          QSW_BACKOFF: "2.0"
          QSW_PURE_MAX_WAIT: "1800"
          QSW_RUN_NAME: "all"
        run: |
          echo "[SANITY] Launching actions_runner.py…"
          ls -la
          python actions_runner.py

      - name: Upload artifacts to GitHub (always)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            QSW_runs/**/_status/**
            QSW_runs/**/results_summary.csv
            QSW_runs/**/metadata.json
            QSW_runs/**/manifest.json
            QSW_runs/**/raw/**

      - name: Upload to Cloudflare R2 (timestamp + run id)
        if: ${{ always() }}
        env:
          AWS_ACCESS_KEY_ID:     ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET:             ${{ secrets.R2_BUCKET }}
          R2_ACCOUNT_ID:         ${{ secrets.R2_ACCOUNT_ID }}   # just the account id (no https)
          R2_PREFIX:             ${{ secrets.R2_PREFIX }}        # optional
        run: |
          echo "[INFO] Uploading QSW_runs/* to R2…"
          TS=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          DEST="${R2_PREFIX:+$R2_PREFIX/}collector-runs/${TS}-run${{ github.run_id }}/"
          echo "[INFO] Destination: s3://$R2_BUCKET/$DEST"
          if [ -d "QSW_runs" ]; then
            aws s3 cp QSW_runs "s3://$R2_BUCKET/$DEST" --recursive \
              --endpoint-url "https://$R2_ACCOUNT_ID.r2.cloudflarestorage.com"
          else
            echo "[WARN] QSW_runs directory not found; nothing to upload."
          fi
