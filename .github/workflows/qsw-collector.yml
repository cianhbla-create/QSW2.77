name: QSW Collector

on:
  workflow_dispatch:

jobs:
  collect:
    runs-on: ubuntu-latest
    timeout-minutes: 720  # ← 12h so pure-QRNG runs don't get canceled

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install boto3 reportlab requests numpy pandas

      - name: Validate R2 secrets
        run: |
          test -n "${{ secrets.S3_ENDPOINT_URL }}" || (echo "Missing S3_ENDPOINT_URL" && exit 1)
          test -n "${{ secrets.S3_BUCKET }}"       || (echo "Missing S3_BUCKET" && exit 1)
          test -n "${{ secrets.AWS_ACCESS_KEY_ID }}" || (echo "Missing AWS_ACCESS_KEY_ID" && exit 1)
          test -n "${{ secrets.AWS_SECRET_ACCESS_KEY }}" || (echo "Missing AWS_SECRET_ACCESS_KEY" && exit 1)
        shell: bash

      - name: R2 preflight — write test object
        run: |
          python - <<'PY'
          import os, time
          import boto3
          from botocore.client import Config

          ep   = os.environ["S3_ENDPOINT_URL"].strip().rstrip("/")
          buck = os.environ["S3_BUCKET"]
          pfx  = os.environ.get("S3_PREFIX","autocollect").strip().rstrip("/")
          key  = f"{pfx}/_status/preflight_{int(time.time())}.txt"

          s3 = boto3.client(
              "s3",
              endpoint_url=ep if ep.startswith("http") else "https://" + ep,
              aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
              aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
              region_name=os.environ.get("S3_REGION","auto"),
              config=Config(signature_version="s3v4", s3={"addressing_style":"virtual"}),
          )
          body = f"hello from actions at {time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())}\n"
          s3.put_object(Bucket=buck, Key=key, Body=body.encode(), ContentType="text/plain")
          print("Wrote:", f"s3://{buck}/{key}")
          PY
        env:
          S3_ENDPOINT_URL: ${{ secrets.S3_ENDPOINT_URL }}
          S3_BUCKET:       ${{ secrets.S3_BUCKET }}
          S3_PREFIX:       ${{ secrets.S3_PREFIX }}
          S3_REGION:       "auto"
          AWS_ACCESS_KEY_ID:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Run one collection pass
        env:
          # R2
          S3_ENDPOINT_URL:        ${{ secrets.S3_ENDPOINT_URL }}
          S3_REGION:              "auto"
          S3_BUCKET:              ${{ secrets.S3_BUCKET }}
          S3_PREFIX:              ${{ secrets.S3_PREFIX }}
          AWS_ACCESS_KEY_ID:      ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY:  ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          # Experiment knobs
          QSW_TRIALS: "3000"                # ← per condition
          QSW_TRIALS_MODE: "per_condition"  # or "total"
          QSW_PHASE_LEN: "8"
          QSW_MAX_STEPS: "30"
          QSW_CONDITIONS: "quantum,pseudo,deterministic"
          # QRNG client tuning
          QSW_QRNG_PROVIDERS: "anu"
          QSW_PURE: "1"              # 1=pure, waits instead of falling back
          QSW_BATCH: "4096"          # ← bigger batches
          QSW_LOW_WATERMARK: "2048"  # ← keep buffer topped up
          QSW_MAX_RETRIES: "20"
          QSW_BACKOFF: "2.0"
          QSW_PURE_MAX_WAIT: "3600"
          # Optional tag
          QSW_RUN_NAME: "all"
        run: |
          python actions_runner.py

      - name: Upload logs/artifacts (always)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: collector-logs
          path: |
            QSW_runs/**/_status/**
            QSW_runs/**/results_summary.csv
            QSW_runs/**/metadata.json
            QSW_runs/**/manifest.json
            QSW_runs/**/raw/**
